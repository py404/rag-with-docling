{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9vCSgGj08Lw"
      },
      "source": [
        "# Exploring Docling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBo5IPx5_SsZ"
      },
      "source": [
        "## Docling Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJxUhSK4-vW1"
      },
      "source": [
        "<img src=\"https://docling-project.github.io/docling/assets/docling_arch.png\" />\n",
        "\n",
        "<img src=\"https://codecut.ai/wp-content/uploads/2025/07/image-with-caption.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDYpjjKV3gp0"
      },
      "source": [
        "## Ingest a document\n",
        "\n",
        "Lets ingest a PDF document and see how docling pipeline works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVlZ8DgiQ3mV"
      },
      "outputs": [],
      "source": [
        "report_url = \"https://arxiv.org/pdf/2408.09869\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrCmC_sflRSa",
        "outputId": "d9fde4ed-908e-4a7b-e675-f802b3cf0406"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import (\n",
        "    PdfPipelineOptions,\n",
        ")\n",
        "from docling.datamodel.settings import settings\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "\n",
        "# Explicitly set the accelerator\n",
        "# ------------------------------\n",
        "## Auto selection\n",
        "# accelerator_options = AcceleratorOptions(\n",
        "#     num_threads=8, device=AcceleratorDevice.AUTO\n",
        "# )\n",
        "\n",
        "## CPU\n",
        "# accelerator_options = AcceleratorOptions(\n",
        "#     num_threads=8, device=AcceleratorDevice.CPU\n",
        "# )\n",
        "\n",
        "## Mac\n",
        "accelerator_options = AcceleratorOptions(\n",
        "    num_threads=8, device=AcceleratorDevice.MPS\n",
        ")\n",
        "\n",
        "## NVidia GPU\n",
        "# accelerator_options = AcceleratorOptions(\n",
        "#     num_threads=8, device=AcceleratorDevice.CUDA\n",
        "# )\n",
        "\n",
        "# easyocr doesnt support cuda:N allocation, defaults to cuda:0\n",
        "# accelerator_options = AcceleratorOptions(num_threads=8, device=\"cuda:1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Building the pipeline\n",
        "# ------------------------------\n",
        "pipeline_options = PdfPipelineOptions()\n",
        "pipeline_options.accelerator_options = accelerator_options\n",
        "pipeline_options.do_ocr = True\n",
        "pipeline_options.do_table_structure = True\n",
        "pipeline_options.table_structure_options.do_cell_matching = True\n",
        "\n",
        "converter = DocumentConverter(\n",
        "    format_options={\n",
        "        InputFormat.PDF: PdfFormatOption(\n",
        "            pipeline_options=pipeline_options,\n",
        "        )\n",
        "    }\n",
        ")\n",
        "\n",
        "# Enable the profiling to measure the time spent\n",
        "settings.debug.profile_pipeline_timings = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converting a document\n",
        "# ------------------------------\n",
        "conversion_result = converter.convert(report_url)\n",
        "\n",
        "doc = conversion_result.document\n",
        "\n",
        "# List with total time per document\n",
        "doc_conversion_secs = conversion_result.timings[\"pipeline_total\"].times\n",
        "\n",
        "# Export to markdown and print it\n",
        "# ---------------------------------\n",
        "md = doc.export_to_markdown()\n",
        "\n",
        "with open(\"output.md\", \"w\") as f:\n",
        "    f.write(md)\n",
        "# print(md)\n",
        "# print(f\"Conversion secs: {doc_conversion_secs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "collapsed": true,
        "id": "c_KNEq4NEP3P",
        "outputId": "353d038b-59c7-4bb0-9dde-40560f477fdd"
      },
      "outputs": [],
      "source": [
        "# Print scores and grades\n",
        "#\n",
        "# ConversionResult data type: https://docling-project.github.io/docling/reference/document_converter/#docling.document_converter.ConversionResult\n",
        "#\n",
        "# ------------------------------\n",
        "# conversion_result.confidence.model_dump_json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrmKQ2RbGzHc"
      },
      "source": [
        "## Confidence Scores\n",
        "\n",
        "Users can and should safely focus on the document-level grade fields — `mean_grade` and `low_grade` — to assess overall conversion quality. Numerical scores are used internally and are for informational purposes only; their computation and weighting may change in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cWQ4kMO5G0nc",
        "outputId": "b14e43c4-1b85-480e-a7b4-7e645516156c"
      },
      "outputs": [],
      "source": [
        "conversion_result.confidence.mean_grade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "_gYgvXnwlgLq",
        "outputId": "daf9bba5-c297-408b-caf3-460600c28610"
      },
      "outputs": [],
      "source": [
        "conversion_result.confidence.low_grade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbEu2DEX5NbN"
      },
      "source": [
        "## Simple RAG with Milvus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VaJ09oaAffN",
        "outputId": "420da507-275d-4ff4-d345-7c734332ccf5"
      },
      "outputs": [],
      "source": [
        "from pymilvus import Collection, CollectionSchema, DataType, FieldSchema, connections, utility\n",
        "from docling.chunking import HybridChunker\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from loguru import logger\n",
        "\n",
        "# Connect to Milvus Lite\n",
        "connections.connect(uri='rag.db')\n",
        "\n",
        "# Define schema for Milvus collection\n",
        "fields = [\n",
        "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
        "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
        "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=384)  # Assuming a model with 384 dimensions\n",
        "]\n",
        "schema = CollectionSchema(fields, \"docling_rag_collection\")\n",
        "\n",
        "# Create collection\n",
        "collection_name = \"docling_rag_collection\"\n",
        "if utility.has_collection(collection_name):\n",
        "    utility.drop_collection(collection_name)\n",
        "collection = Collection(collection_name, schema)\n",
        "\n",
        "# Index the collection\n",
        "index_params = {\n",
        "    \"index_type\": \"IVF_FLAT\",\n",
        "    \"metric_type\": \"L2\",\n",
        "    \"params\": {\"nlist\": 1024},\n",
        "}\n",
        "collection.create_index(\"vector\", index_params)\n",
        "\n",
        "# Chunk the document text using Docling's HybridChunker\n",
        "## Initialize the chunker\n",
        "chunker = HybridChunker()\n",
        "## Use the chunker on the Docling document object\n",
        "chunks_iterator = chunker.chunk(doc)\n",
        "## Convert the iterator to a list\n",
        "chunks = list(chunks_iterator)\n",
        "\n",
        "# Debugging: Print the number of chunks and the first few chunks\n",
        "logger.info(f\"Number of chunks: {len(chunks)}\")\n",
        "if chunks:\n",
        "    logger.info(\"First few chunks:\")\n",
        "    for i, chunk in enumerate(chunks[:5]):\n",
        "        logger.info(f\"Chunk {i}: {chunk.text[:100]}...\") # Print first 100 characters\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "# You can choose a different model based on your needs\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings using the loaded model\n",
        "def create_embeddings(text):\n",
        "    # Ensure the embedding is a list of floats\n",
        "    return model.encode(text).tolist()\n",
        "\n",
        "# Debugging: Print the structure of the entities before insertion\n",
        "texts = [chunk.text for chunk in chunks]\n",
        "embeddings = [create_embeddings(chunk.text) for chunk in chunks]\n",
        "\n",
        "# Convert embeddings to a NumPy array\n",
        "embeddings_np = np.array(embeddings, dtype=np.float32)\n",
        "\n",
        "# Debugging: Print the shape of the embeddings array\n",
        "logger.info(f\"Shape of embeddings array: {embeddings_np.shape}\")\n",
        "\n",
        "entities = [texts, embeddings_np] # Use the NumPy array for embeddings\n",
        "\n",
        "logger.info(f\"Structure of entities: {[len(e) for e in entities]}\")\n",
        "if entities and len(entities) > 0 and len(entities[0]) > 0:\n",
        "    logger.info(f\"First text entity: {entities[0][0][:100]}...\")\n",
        "    if len(entities) > 1 and len(entities[1]) > 0:\n",
        "        logger.info(f\"First embedding entity: {entities[1][0][:10]}...\")\n",
        "\n",
        "\n",
        "# Insert chunks and embeddings into Milvus\n",
        "collection.insert(entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfo0nlbwUOWa"
      },
      "source": [
        "### Example search query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q28lyIW3UIaA",
        "outputId": "77db7bc2-0901-4dea-a94f-c728e0ad0d59"
      },
      "outputs": [],
      "source": [
        "# Load collection for search\n",
        "collection.load()\n",
        "\n",
        "query_text = input(\"Enter your query:\")\n",
        "query_embedding = create_embeddings(query_text)\n",
        "\n",
        "# Perform search\n",
        "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
        "results = collection.search([query_embedding], \"vector\", search_params, limit=3)\n",
        "\n",
        "# Aggregate and print search results in a single output\n",
        "logger.info(\"Search Results:\")\n",
        "all_results_text = \"\"\n",
        "for hits in results:\n",
        "    for hit in hits:\n",
        "        # Retrieve the original text based on the ID\n",
        "        result_entity = collection.query(expr=f\"id == {hit.id}\", output_fields=[\"text\"])\n",
        "        if result_entity:\n",
        "            all_results_text += f\"ID: {hit.id}, Score: {hit.score}\\n\"\n",
        "            all_results_text += f\"Text: {result_entity[0]['text']}\\n\\n\"\n",
        "\n",
        "print(all_results_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
